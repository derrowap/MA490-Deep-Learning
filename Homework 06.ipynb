{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "# Homework 6\n",
    "Austin Derrow-Pinion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dimensions:\n",
      "training input: (1000, 28, 28)  output: (1000,)\n",
      "testing input:  (1000, 28, 28)  output: (1000,)\n"
     ]
    }
   ],
   "source": [
    "# load MNIST data\n",
    "MNIST = np.load('./Data/MNIST_train_1000.npz')\n",
    "images_train = MNIST['train_images']\n",
    "labels_train = MNIST['train_labels']\n",
    "print('dimensions:')\n",
    "print('training input:', images_train.shape, ' output:', labels_train.shape)\n",
    "MNIST = np.load('./Data/MNIST_test_1000.npz')\n",
    "images_test = MNIST['test_images']\n",
    "labels_test = MNIST['test_labels']\n",
    "print('testing input: ', images_test.shape, ' output:', labels_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dimensions:\n",
      "training input: (1000, 784)  output: (1000, 2)\n",
      "testing  input: (1000, 784)  output: (1000, 2)\n"
     ]
    }
   ],
   "source": [
    "# reshaping image tensors to a matrix \n",
    "inputs_train = images_train.reshape(-1, 28 * 28)\n",
    "inputs_test  = images_test.reshape(-1, 28 * 28)\n",
    "\n",
    "# one-hot-encode labels\n",
    "outputs_train = OneHotEncoder(sparse=False).fit_transform(labels_train.reshape(-1, 1))\n",
    "outputs_test  = OneHotEncoder(sparse=False).fit_transform(labels_test.reshape(-1, 1))\n",
    "print('dimensions:')\n",
    "print('training input:', inputs_train.shape, ' output:', outputs_train.shape)\n",
    "print('testing  input:', inputs_test.shape, ' output:', outputs_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test(sess, optimizer, CE, p_train, p_test):\n",
    "    # training\n",
    "    error_train = []\n",
    "    error_test  = []\n",
    "    for i in range(2000):\n",
    "        (_,ce,pa,pb) = sess.run([optimizer,CE,p_train,p_test])\n",
    "        pa = np.argmax(pa,axis=1)\n",
    "        pa[pa==0] = 4\n",
    "        pa[pa==1] = 9 \n",
    "        labels_train_pred = pa\n",
    "        pb = np.argmax(pb,axis=1)\n",
    "        pb[pb==0] = 4\n",
    "        pb[pb==1] = 9    \n",
    "        labels_test_pred = pb\n",
    "        err_train = 1 - accuracy_score(labels_train,labels_train_pred)\n",
    "        err_test  = 1 - accuracy_score(labels_test,labels_test_pred)\n",
    "        error_train.append(err_train)\n",
    "        error_test.append(err_test)\n",
    "        if (i+1) % 100 == 0:\n",
    "            print('step = %-5d cross_entropy = %-10.5f training = %-10.5f testing = %-10.5f' % \n",
    "                  (i+1,ce,err_train,err_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) Use logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define TensorFlow computation graph\n",
    "\n",
    "x = tf.constant(inputs_train, dtype='float32', shape=[1000, 784])\n",
    "x_test = tf.constant(inputs_test, dtype='float32', shape=[1000, 784])\n",
    "y = tf.constant(outputs_train, dtype='float32', shape=[1000, 2])\n",
    "\n",
    "w = tf.Variable(tf.truncated_normal([784, 2], stddev=0.1))\n",
    "b = tf.Variable(tf.truncated_normal([1, 2], stddev=0.1))\n",
    "\n",
    "logits = tf.matmul(x, w) + b\n",
    "p_train = tf.nn.softmax(logits)\n",
    "p_test = tf.nn.softmax(tf.matmul(x_test, w) + b)\n",
    "\n",
    "CE = tf.reduce_mean(tf.reduce_sum(-y * tf.log(tf.nn.softmax(logits)), reduction_indices=[1]))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer().minimize(CE)\n",
    "\n",
    "# initialization of variables\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "# initialize a computation session\n",
    "sess = tf.Session()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 100   cross_entropy = 0.20544    training = 0.04000    testing = 0.05500   \n",
      "step = 200   cross_entropy = 0.13411    training = 0.02200    testing = 0.04700   \n",
      "step = 300   cross_entropy = 0.10347    training = 0.01800    testing = 0.04500   \n",
      "step = 400   cross_entropy = 0.08542    training = 0.01600    testing = 0.04300   \n",
      "step = 500   cross_entropy = 0.07306    training = 0.01300    testing = 0.04000   \n",
      "step = 600   cross_entropy = 0.06381    training = 0.01200    testing = 0.03900   \n",
      "step = 700   cross_entropy = 0.05651    training = 0.01100    testing = 0.03800   \n",
      "step = 800   cross_entropy = 0.05052    training = 0.01000    testing = 0.03700   \n",
      "step = 900   cross_entropy = 0.04549    training = 0.00600    testing = 0.03600   \n",
      "step = 1000  cross_entropy = 0.04120    training = 0.00500    testing = 0.03500   \n",
      "step = 1100  cross_entropy = 0.03750    training = 0.00500    testing = 0.03500   \n",
      "step = 1200  cross_entropy = 0.03427    training = 0.00300    testing = 0.03500   \n",
      "step = 1300  cross_entropy = 0.03143    training = 0.00200    testing = 0.03600   \n",
      "step = 1400  cross_entropy = 0.02892    training = 0.00200    testing = 0.03600   \n",
      "step = 1500  cross_entropy = 0.02668    training = 0.00200    testing = 0.03600   \n",
      "step = 1600  cross_entropy = 0.02467    training = 0.00100    testing = 0.03700   \n",
      "step = 1700  cross_entropy = 0.02286    training = 0.00000    testing = 0.03700   \n",
      "step = 1800  cross_entropy = 0.02122    training = 0.00000    testing = 0.03800   \n",
      "step = 1900  cross_entropy = 0.01974    training = 0.00000    testing = 0.03900   \n",
      "step = 2000  cross_entropy = 0.01839    training = 0.00000    testing = 0.03900   \n"
     ]
    }
   ],
   "source": [
    "test(sess, optimizer, CE, p_train, p_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### (b) Use a neural network with one hidden layer of 1024 nodes. Do not use a RELU layer between the linear layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define TensorFlow computation graph\n",
    "epsilon = 1e-15\n",
    "\n",
    "w1 = tf.Variable(tf.truncated_normal([784, 1024], stddev=0.1))\n",
    "b1 = tf.Variable(tf.truncated_normal([1, 1024], stddev=0.1))\n",
    "\n",
    "w2 = tf.Variable(tf.truncated_normal([1024, 2], stddev=0.1))\n",
    "b2 = tf.Variable(tf.truncated_normal([1, 2], stddev=0.1))\n",
    "\n",
    "y1_train = tf.matmul(x, w1) + b1\n",
    "y1_test = tf.matmul(x_test, w1) + b1\n",
    "\n",
    "y2_train = tf.matmul(y1_train, w2) + b2\n",
    "y2_test = tf.matmul(y1_test, w2) + b2\n",
    "\n",
    "p_train = tf.nn.softmax(y2_train) + epsilon\n",
    "p_test = tf.nn.softmax(y2_test) + epsilon\n",
    "\n",
    "CE = tf.reduce_mean(tf.reduce_sum(-y * tf.log(p_train), reduction_indices=[1]))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer().minimize(CE)\n",
    "\n",
    "# initialization of variables\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "# initialize a computation session\n",
    "sess = tf.Session()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 100   cross_entropy = 0.03213    training = 0.00700    testing = 0.03400   \n",
      "step = 200   cross_entropy = 0.01645    training = 0.00000    testing = 0.03400   \n",
      "step = 300   cross_entropy = 0.00944    training = 0.00000    testing = 0.03400   \n",
      "step = 400   cross_entropy = 0.00577    training = 0.00000    testing = 0.03800   \n",
      "step = 500   cross_entropy = 0.00371    training = 0.00000    testing = 0.03800   \n",
      "step = 600   cross_entropy = 0.00250    training = 0.00000    testing = 0.03900   \n",
      "step = 700   cross_entropy = 0.00175    training = 0.00000    testing = 0.04000   \n",
      "step = 800   cross_entropy = 0.00127    training = 0.00000    testing = 0.04200   \n",
      "step = 900   cross_entropy = 0.00096    training = 0.00000    testing = 0.04200   \n",
      "step = 1000  cross_entropy = 0.00074    training = 0.00000    testing = 0.04200   \n",
      "step = 1100  cross_entropy = 0.00058    training = 0.00000    testing = 0.04200   \n",
      "step = 1200  cross_entropy = 0.00047    training = 0.00000    testing = 0.04300   \n",
      "step = 1300  cross_entropy = 0.00038    training = 0.00000    testing = 0.04300   \n",
      "step = 1400  cross_entropy = 0.00031    training = 0.00000    testing = 0.04200   \n",
      "step = 1500  cross_entropy = 0.00026    training = 0.00000    testing = 0.04300   \n",
      "step = 1600  cross_entropy = 0.00022    training = 0.00000    testing = 0.04300   \n",
      "step = 1700  cross_entropy = 0.00019    training = 0.00000    testing = 0.04300   \n",
      "step = 1800  cross_entropy = 0.00016    training = 0.00000    testing = 0.04300   \n",
      "step = 1900  cross_entropy = 0.00014    training = 0.00000    testing = 0.04300   \n",
      "step = 2000  cross_entropy = 0.00012    training = 0.00000    testing = 0.04300   \n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "test(sess, optimizer, CE, p_train, p_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c) Use a neural network with one hidden layer of 1024 nodes. Separate the linear layers with a RELU layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define TensorFlow computation graph\n",
    "\n",
    "w1 = tf.Variable(tf.truncated_normal([784, 1024], stddev=0.1))\n",
    "b1 = tf.Variable(tf.truncated_normal([1, 1024], stddev=0.1))\n",
    "\n",
    "w2 = tf.Variable(tf.truncated_normal([1024, 2], stddev=0.1))\n",
    "b2 = tf.Variable(tf.truncated_normal([1, 2], stddev=0.1))\n",
    "\n",
    "y1_train = tf.nn.relu(tf.matmul(x, w1) + b1)\n",
    "y1_test = tf.nn.relu(tf.matmul(x_test, w1) + b1)\n",
    "\n",
    "y2_train = tf.matmul(y1_train, w2) + b2\n",
    "y2_test = tf.matmul(y1_test, w2) + b2\n",
    "\n",
    "p_train = tf.nn.softmax(y2_train)\n",
    "p_test = tf.nn.softmax(y2_test)\n",
    "\n",
    "CE = tf.reduce_mean(tf.reduce_sum(-y * tf.log(p_train), reduction_indices=[1]))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer().minimize(CE)\n",
    "\n",
    "# initialization of variables\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "# initialize a computation session\n",
    "sess = tf.Session()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 100   cross_entropy = 0.00624    training = 0.00000    testing = 0.02000   \n",
      "step = 200   cross_entropy = 0.00115    training = 0.00000    testing = 0.02300   \n",
      "step = 300   cross_entropy = 0.00043    training = 0.00000    testing = 0.02300   \n",
      "step = 400   cross_entropy = 0.00022    training = 0.00000    testing = 0.02300   \n",
      "step = 500   cross_entropy = 0.00013    training = 0.00000    testing = 0.02400   \n",
      "step = 600   cross_entropy = 0.00009    training = 0.00000    testing = 0.02400   \n",
      "step = 700   cross_entropy = 0.00006    training = 0.00000    testing = 0.02400   \n",
      "step = 800   cross_entropy = 0.00005    training = 0.00000    testing = 0.02400   \n",
      "step = 900   cross_entropy = 0.00004    training = 0.00000    testing = 0.02400   \n",
      "step = 1000  cross_entropy = 0.00003    training = 0.00000    testing = 0.02400   \n",
      "step = 1100  cross_entropy = 0.00002    training = 0.00000    testing = 0.02400   \n",
      "step = 1200  cross_entropy = 0.00002    training = 0.00000    testing = 0.02400   \n",
      "step = 1300  cross_entropy = 0.00002    training = 0.00000    testing = 0.02400   \n",
      "step = 1400  cross_entropy = 0.00001    training = 0.00000    testing = 0.02400   \n",
      "step = 1500  cross_entropy = 0.00001    training = 0.00000    testing = 0.02400   \n",
      "step = 1600  cross_entropy = 0.00001    training = 0.00000    testing = 0.02400   \n",
      "step = 1700  cross_entropy = 0.00001    training = 0.00000    testing = 0.02400   \n",
      "step = 1800  cross_entropy = 0.00001    training = 0.00000    testing = 0.02400   \n",
      "step = 1900  cross_entropy = 0.00001    training = 0.00000    testing = 0.02400   \n",
      "step = 2000  cross_entropy = 0.00001    training = 0.00000    testing = 0.02400   \n"
     ]
    }
   ],
   "source": [
    "test(sess, optimizer, CE, p_train, p_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (d) Use a neural network with one hidden layer of 1024 nodes. Separate the linear layers with a RELU layer. Use L2 regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define TensorFlow computation graph\n",
    "\n",
    "w1 = tf.Variable(tf.truncated_normal([784, 1024], stddev=0.1))\n",
    "b1 = tf.Variable(tf.truncated_normal([1, 1024], stddev=0.1))\n",
    "\n",
    "w2 = tf.Variable(tf.truncated_normal([1024, 2], stddev=0.1))\n",
    "b2 = tf.Variable(tf.truncated_normal([1, 2], stddev=0.1))\n",
    "\n",
    "y1_train = tf.nn.relu(tf.matmul(x, w1) + b1)\n",
    "y1_test = tf.nn.relu(tf.matmul(x_test, w1) + b1)\n",
    "\n",
    "y2_train = tf.matmul(y1_train, w2) + b2\n",
    "y2_test = tf.matmul(y1_test, w2) + b2\n",
    "\n",
    "p_train = tf.nn.softmax(y2_train)\n",
    "p_test = tf.nn.softmax(y2_test)\n",
    "\n",
    "beta = 0.00001 # 0.0200\n",
    "CE = tf.reduce_mean(tf.reduce_sum(-y * tf.log(p_train)\n",
    "                                  + beta * (tf.reduce_mean(tf.reduce_sum(w1 ** 2))\n",
    "                                            + tf.reduce_mean(tf.reduce_sum(w2 ** 2))),\n",
    "                                  reduction_indices=[1]))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer().minimize(CE)\n",
    "\n",
    "# initialization of variables\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "# initialize a computation session\n",
    "sess = tf.Session()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 100   cross_entropy = 0.08053    training = 0.00000    testing = 0.02300   \n",
      "step = 200   cross_entropy = 0.06183    training = 0.00000    testing = 0.02500   \n",
      "step = 300   cross_entropy = 0.05394    training = 0.00000    testing = 0.02500   \n",
      "step = 400   cross_entropy = 0.04856    training = 0.00000    testing = 0.02400   \n",
      "step = 500   cross_entropy = 0.04429    training = 0.00000    testing = 0.02400   \n",
      "step = 600   cross_entropy = 0.04071    training = 0.00000    testing = 0.02400   \n",
      "step = 700   cross_entropy = 0.03762    training = 0.00000    testing = 0.02500   \n",
      "step = 800   cross_entropy = 0.03490    training = 0.00000    testing = 0.02600   \n",
      "step = 900   cross_entropy = 0.03249    training = 0.00000    testing = 0.02600   \n",
      "step = 1000  cross_entropy = 0.03032    training = 0.00000    testing = 0.02500   \n",
      "step = 1100  cross_entropy = 0.02835    training = 0.00000    testing = 0.02600   \n",
      "step = 1200  cross_entropy = 0.02655    training = 0.00000    testing = 0.02600   \n",
      "step = 1300  cross_entropy = 0.02489    training = 0.00000    testing = 0.02600   \n",
      "step = 1400  cross_entropy = 0.02336    training = 0.00000    testing = 0.02600   \n",
      "step = 1500  cross_entropy = 0.02195    training = 0.00000    testing = 0.02800   \n",
      "step = 1600  cross_entropy = 0.02063    training = 0.00000    testing = 0.02800   \n",
      "step = 1700  cross_entropy = 0.01940    training = 0.00000    testing = 0.02900   \n",
      "step = 1800  cross_entropy = 0.01825    training = 0.00000    testing = 0.02900   \n",
      "step = 1900  cross_entropy = 0.01717    training = 0.00000    testing = 0.02900   \n",
      "step = 2000  cross_entropy = 0.01616    training = 0.00000    testing = 0.02900   \n"
     ]
    }
   ],
   "source": [
    "test(sess, optimizer, CE, p_train, p_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "default_view": {},
   "name": "2_fullyconnected.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
