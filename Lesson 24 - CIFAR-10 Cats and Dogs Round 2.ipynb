{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 24 - CIFAR-10 Cats and Dogs Round 2\n",
    "Austin Derrow-Pinion CM 208"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import scale\n",
    "import time\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['airplane' 'automobile' 'bird' 'cat' 'deer' 'dog' 'frog' 'horse' 'ship'\n",
      " 'truck']\n",
      "images_train shape: (6000, 32, 32, 3)\n",
      "labels_train shape: (6000,)\n",
      "images_valid shape: (2000, 32, 32, 3)\n",
      "labels_valid shape: (2000,)\n",
      "images_test shape: (2000, 32, 32, 3)\n",
      "labels_test shape: (2000,)\n"
     ]
    }
   ],
   "source": [
    "# load CIFAR-10 Cats and Dogs split data set\n",
    "CIFAR10 = np.load('./Data/CIFAR-10_cats_dogs_split.npz')\n",
    "\n",
    "# label names\n",
    "label_names = CIFAR10['label_names']\n",
    "\n",
    "# training data\n",
    "images_train = CIFAR10['images_train']\n",
    "labels_train = CIFAR10['labels_train']\n",
    "\n",
    "# validation data\n",
    "images_valid = CIFAR10['images_valid']\n",
    "labels_valid = CIFAR10['labels_valid']\n",
    "\n",
    "# test data\n",
    "images_test = CIFAR10['images_test']\n",
    "labels_test = CIFAR10['labels_test']\n",
    "\n",
    "# labels and shapes of data\n",
    "print(label_names)\n",
    "print('images_train shape:', images_train.shape)\n",
    "print('labels_train shape:', labels_train.shape)\n",
    "print('images_valid shape:', images_valid.shape)\n",
    "print('labels_valid shape:', labels_valid.shape)\n",
    "print('images_test shape:', images_test.shape)\n",
    "print('labels_test shape:', labels_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.5/site-packages/sklearn/preprocessing/data.py:167: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n",
      "  warnings.warn(\"Numerical issues were encountered \"\n",
      "/opt/anaconda3/lib/python3.5/site-packages/sklearn/preprocessing/data.py:184: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. \n",
      "  warnings.warn(\"Numerical issues were encountered \"\n"
     ]
    }
   ],
   "source": [
    "# scale data\n",
    "image_size = 32*32*3\n",
    "\n",
    "images_train_s = scale(np.reshape(np.array(images_train, dtype='float32'), [-1, image_size]), axis=1)\n",
    "images_valid_s = scale(np.reshape(np.array(images_valid, dtype='float32'), [-1, image_size]), axis=1)\n",
    "images_test_s = scale(np.reshape(np.array(images_test, dtype='float32'), [-1, image_size]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Tensorflow computational graph\n",
    "x = tf.placeholder(tf.float32, shape=[None, image_size])\n",
    "y = tf.placeholder(tf.float32, shape=[None])\n",
    "y_one_hot = tf.one_hot(tf.to_int32(y), depth=10, on_value=1.0, off_value=0.0, dtype=tf.float32)\n",
    "\n",
    "W = tf.Variable(tf.truncated_normal([image_size, 10], dtype=tf.float32, seed=0, stddev=0.1))\n",
    "b = tf.Variable(tf.truncated_normal([10], dtype=tf.float32, seed=0, stddev=0.1))\n",
    "\n",
    "logits = tf.matmul(x, W) + b\n",
    "\n",
    "CE = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, y_one_hot))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(CE)\n",
    "\n",
    "y_pred = tf.argmax(tf.nn.softmax(logits), dimension=1)\n",
    "accuracy = 1.0 - tf.contrib.metrics.accuracy(tf.to_int32(y_pred), tf.to_int32(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def minibatches(x, y, batchsize):\n",
    "    assert x.shape[0] == y.shape[0]\n",
    "    indices = np.arange(x.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    for i in range(0, x.shape[0] - batchsize + 1, batchsize):\n",
    "        excerpt = indices[i:i + batchsize]\n",
    "        yield np.array(x[excerpt], dtype='float32'), np.array(y[excerpt], dtype='int32')\n",
    "    \n",
    "    leftOver = (x.shape[0] - batchsize) % batchsize\n",
    "    if leftOver != 0:\n",
    "        excerpt = indices[len(indices) - leftOver:]\n",
    "        yield np.array(x[excerpt], dtype='float32'), np.array(y[excerpt], dtype='int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH = 100\n",
      "    training cross-entropy = 0.84290\n",
      "  validation cross-entropy = 1.19754\n",
      "       training error rate = 0.332\n",
      "     validation error rate = 0.489\n",
      "      total time (minutes) = 0.3\n",
      "EPOCH = 200\n",
      "    training cross-entropy = 0.68290\n",
      "  validation cross-entropy = 1.07068\n",
      "       training error rate = 0.297\n",
      "     validation error rate = 0.476\n",
      "      total time (minutes) = 0.6\n",
      "EPOCH = 300\n",
      "    training cross-entropy = 0.59538\n",
      "  validation cross-entropy = 0.98755\n",
      "       training error rate = 0.263\n",
      "     validation error rate = 0.456\n",
      "      total time (minutes) = 0.8\n",
      "EPOCH = 400\n",
      "    training cross-entropy = 0.56376\n",
      "  validation cross-entropy = 0.95238\n",
      "       training error rate = 0.266\n",
      "     validation error rate = 0.454\n",
      "      total time (minutes) = 1.1\n",
      "EPOCH = 500\n",
      "    training cross-entropy = 0.53360\n",
      "  validation cross-entropy = 0.92521\n",
      "       training error rate = 0.248\n",
      "     validation error rate = 0.453\n",
      "      total time (minutes) = 1.3\n",
      "EPOCH = 600\n",
      "    training cross-entropy = 0.51747\n",
      "  validation cross-entropy = 0.92164\n",
      "       training error rate = 0.245\n",
      "     validation error rate = 0.465\n",
      "      total time (minutes) = 1.5\n",
      "EPOCH = 700\n",
      "    training cross-entropy = 0.49926\n",
      "  validation cross-entropy = 0.90578\n",
      "       training error rate = 0.235\n",
      "     validation error rate = 0.451\n",
      "      total time (minutes) = 1.7\n",
      "EPOCH = 800\n",
      "    training cross-entropy = 0.49182\n",
      "  validation cross-entropy = 0.91319\n",
      "       training error rate = 0.226\n",
      "     validation error rate = 0.449\n",
      "      total time (minutes) = 1.9\n",
      "EPOCH = 900\n",
      "    training cross-entropy = 0.49875\n",
      "  validation cross-entropy = 0.91116\n",
      "       training error rate = 0.241\n",
      "     validation error rate = 0.427\n",
      "      total time (minutes) = 2.1\n",
      "EPOCH = 1000\n",
      "    training cross-entropy = 0.47750\n",
      "  validation cross-entropy = 0.90891\n",
      "       training error rate = 0.225\n",
      "     validation error rate = 0.442\n",
      "      total time (minutes) = 2.3\n"
     ]
    }
   ],
   "source": [
    "init = tf.initialize_all_variables()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    num_epochs = 0\n",
    "    EPOCHS = 1000\n",
    "    batch_size = 1000\n",
    "    starting_time = time.time()\n",
    "    \n",
    "    while num_epochs < EPOCHS:\n",
    "        \n",
    "        for batch in minibatches(images_train_s, labels_train, batch_size):\n",
    "            batch_x, batch_y = batch\n",
    "            _ = sess.run([optimizer], feed_dict={x: batch_x, y: batch_y})\n",
    "\n",
    "        \n",
    "        num_epochs += 1\n",
    "        if (num_epochs % 100 == 0):\n",
    "            training_CE, training_error = sess.run([CE, accuracy],\n",
    "                                                   feed_dict={x: images_train_s,\n",
    "                                                              y: labels_train})\n",
    "            validation_CE, validation_error = sess.run([CE, accuracy],\n",
    "                                                      feed_dict={x: images_valid_s,\n",
    "                                                                 y: labels_valid})\n",
    "            print(\"EPOCH = {:d}\".format(num_epochs))\n",
    "            print(\"    training cross-entropy = {:.5f}\".format(training_CE))\n",
    "            print(\"  validation cross-entropy = {:.5f}\".format(validation_CE))\n",
    "            print(\"       training error rate = {:.3f}\".format(training_error))\n",
    "            print(\"     validation error rate = {:.3f}\".format(validation_error))\n",
    "            print(\"      total time (minutes) = {:.1f}\".format((time.time() - starting_time) / 60.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
