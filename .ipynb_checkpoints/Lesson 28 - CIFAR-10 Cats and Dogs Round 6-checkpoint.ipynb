{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 28 - CIFAR-10 Cats and Dogs Round 6\n",
    "Austin Derrow-Pinion CM 208"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CIFAR-10 cats and dogs training and validation data set loaded.\n",
      "  training input: (6000, 32, 32, 3)  output: (6000,)\n",
      "validation input: (2000, 32, 32, 3)  output: (2000,)\n"
     ]
    }
   ],
   "source": [
    "# load training, validation and testing data sets\n",
    "CIFAR10 = np.load('./Data/CIFAR-10_cats_dogs_split.npz')\n",
    "label_names  = CIFAR10['label_names']\n",
    "images_train = CIFAR10['images_train']\n",
    "labels_train = CIFAR10['labels_train']\n",
    "images_valid = CIFAR10['images_valid']\n",
    "labels_valid = CIFAR10['labels_valid']\n",
    "\n",
    "print('CIFAR-10 cats and dogs training and validation data set loaded.')\n",
    "print('  training input:',images_train.shape,' output:',labels_train.shape)\n",
    "print('validation input:',images_valid.shape,' output:',labels_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs_train = images_train\n",
    "inputs_valid = images_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data reshaped, scaled and one-hot-encoded:\n",
      "   training input: (6000, 32, 32, 3)  output: (6000, 2)\n",
      " validation input: (2000, 32, 32, 3)  output: (2000, 2)\n"
     ]
    }
   ],
   "source": [
    "# one-hot-encode labels\n",
    "outputs_train = OneHotEncoder(sparse=False).fit_transform(labels_train.reshape(-1,1))\n",
    "outputs_valid = OneHotEncoder(sparse=False).fit_transform(labels_valid.reshape(-1,1))\n",
    "\n",
    "print('Data reshaped, scaled and one-hot-encoded:')\n",
    "print('   training input:',inputs_train.shape,' output:',outputs_train.shape)\n",
    "print(' validation input:',inputs_valid.shape,' output:',outputs_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hyper-parameters\n",
    "EPOCHS = 100\n",
    "N1 = 6000\n",
    "PS = 4\n",
    "DEPTH = 8\n",
    "DEPTH_2 = 8*2\n",
    "LAM = 0.01\n",
    "BS = 500\n",
    "STD = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Tensorflow computation graph\n",
    "x = tf.placeholder(shape=[None, 32, 32, 3], dtype=tf.float32)\n",
    "y = tf.placeholder(shape=[None, 2], dtype=tf.float32)\n",
    "\n",
    "# input --> convLayer 1\n",
    "W0 = tf.Variable(tf.truncated_normal([PS, PS, 3, DEPTH], stddev=STD))\n",
    "b0 = tf.Variable(tf.truncated_normal([DEPTH], stddev=STD))\n",
    "y1 = tf.nn.relu(tf.nn.conv2d(x, W0, [1, 2, 2, 1], padding='SAME') + b0)\n",
    "\n",
    "# convLayer 1 --> convLayer2\n",
    "W1 = tf.Variable(tf.truncated_normal([PS, PS, DEPTH, DEPTH_2], stddev=STD))\n",
    "b1 = tf.Variable(tf.truncated_normal([DEPTH_2], stddev=STD))\n",
    "y2 = tf.nn.relu(tf.nn.conv2d(y1, W1, [1, 2, 2, 1], padding='SAME') + b1)\n",
    "\n",
    "# convLayer2 --> fullyConnected\n",
    "flat_shape = 8 * 8 * DEPTH_2\n",
    "y2_flat = tf.reshape(y2, [-1, flat_shape])\n",
    "W2 = tf.Variable(tf.truncated_normal([flat_shape, 2], stddev=STD))\n",
    "b2 = tf.Variable(tf.truncated_normal([2], stddev=STD))\n",
    "logits = tf.matmul(y2_flat, W2) + b2\n",
    "y_pred = tf.argmax(tf.nn.softmax(logits), 1)\n",
    "\n",
    "l2 = LAM * tf.reduce_mean(tf.nn.l2_loss(W0) + tf.nn.l2_loss(W1) + tf.nn.l2_loss(W2))\n",
    "CE = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(CE + l2)\n",
    "accuracy = 1.0 - tf.contrib.metrics.accuracy(y_pred, tf.argmax(y, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trainNetwork(sess, init):\n",
    "    sess.run(init)\n",
    "    t_start = time.time()\n",
    "    num_batches = np.ceil(N1 / BS)\n",
    "    print('%15s%20s%23s' % (' ','cross-entropy','error-rate'))\n",
    "    print('%15s%12s%12s%12s%12s%12s%12s' % ('epoch','training','validation','training','validation','L2','time (min)'))\n",
    "    for e in np.arange(EPOCHS):\n",
    "        perm = np.random.permutation(N1)\n",
    "        ix = np.arange(0, BS)\n",
    "        e += 1\n",
    "        for batch in np.arange(num_batches):\n",
    "            x_batch = inputs_train[perm[ix], :]\n",
    "            y_batch = outputs_train[perm[ix], :]\n",
    "            sess.run([optimizer],\n",
    "                     feed_dict={x: x_batch, y: y_batch})\n",
    "            ix += BS\n",
    "        if (e % (EPOCHS / 10)) == 0:\n",
    "            CE_train, error_train, l2_loss = sess.run([CE, accuracy, l2],\n",
    "                                         feed_dict={x: inputs_train,\n",
    "                                                    y: outputs_train})\n",
    "            CE_valid, error_valid = sess.run([CE, accuracy],\n",
    "                                         feed_dict={x: inputs_valid,\n",
    "                                                    y: outputs_valid})\n",
    "            total_compute_time = (time.time() - t_start) / 60\n",
    "            print('%7d %7d%12.5f%12.5f%12.3f%12.3f%12f%12.1f' \\\n",
    "                  % (EPOCHS, e, CE_train, CE_valid,\n",
    "                     error_train, error_valid, l2_loss, total_compute_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      cross-entropy             error-rate\n",
      "          epoch    training  validation    training  validation          L2  time (min)\n",
      "    100      10     0.61208     0.64131       0.333       0.373    0.002219         0.2\n",
      "    100      20     0.54286     0.64249       0.277       0.358    0.005635         0.3\n",
      "    100      30     0.48016     0.66319       0.233       0.350    0.009629         0.5\n",
      "    100      40     0.44841     0.69359       0.214       0.335    0.013664         0.6\n",
      "    100      50     0.40333     0.76226       0.185       0.354    0.016978         0.8\n",
      "    100      60     0.37042     0.81106       0.163       0.350    0.019728         1.0\n",
      "    100      70     0.36070     0.84040       0.161       0.350    0.022861         1.1\n",
      "    100      80     0.32011     0.92668       0.136       0.357    0.026228         1.3\n",
      "    100      90     0.30028     1.03825       0.127       0.373    0.030039         1.4\n",
      "    100     100     0.28084     1.11324       0.122       0.363    0.033346         1.6\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "init = tf.initialize_all_variables()\n",
    "trainNetwork(sess, init)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
