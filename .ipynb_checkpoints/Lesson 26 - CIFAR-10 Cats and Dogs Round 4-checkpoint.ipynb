{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 26 - CIFAR-10 Cats and Dogs Round 4\n",
    "Austin Derrow-Pinion CM208"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.metrics import accuracy_score\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CIFAR-10 cats and dogs training and validation data set loaded.\n",
      "  training input: (6000, 32, 32, 3)  output: (6000,)\n",
      "validation input: (2000, 32, 32, 3)  output: (2000,)\n"
     ]
    }
   ],
   "source": [
    "# load training, validation and testing data sets\n",
    "CIFAR10 = np.load('./Data/CIFAR-10_cats_dogs_split.npz')\n",
    "label_names  = CIFAR10['label_names']\n",
    "images_train = CIFAR10['images_train']\n",
    "labels_train = CIFAR10['labels_train']\n",
    "images_valid = CIFAR10['images_valid']\n",
    "labels_valid = CIFAR10['labels_valid']\n",
    "\n",
    "print('CIFAR-10 cats and dogs training and validation data set loaded.')\n",
    "print('  training input:',images_train.shape,' output:',labels_train.shape)\n",
    "print('validation input:',images_valid.shape,' output:',labels_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reshape image tensors to a matrix \n",
    "image_shape = 32*32*3\n",
    "inputs_train = images_train.reshape(-1,image_shape)\n",
    "inputs_valid = images_valid.reshape(-1,image_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# scale data\n",
    "inputs_train = scale(inputs_train.astype('float64'),axis=1)\n",
    "inputs_valid = scale(inputs_valid.astype('float64'),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data reshaped, scaled and one-hot-encoded:\n",
      "   training input: (6000, 3072)  output: (6000, 2)\n",
      " validation input: (2000, 3072)  output: (2000, 2)\n"
     ]
    }
   ],
   "source": [
    "# one-hot-encode labels\n",
    "outputs_train = OneHotEncoder(sparse=False).fit_transform(labels_train.reshape(-1,1))\n",
    "outputs_valid = OneHotEncoder(sparse=False).fit_transform(labels_valid.reshape(-1,1))\n",
    "\n",
    "print('Data reshaped, scaled and one-hot-encoded:')\n",
    "print('   training input:',inputs_train.shape,' output:',outputs_train.shape)\n",
    "print(' validation input:',inputs_valid.shape,' output:',outputs_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hyper-parameters\n",
    "EPOCHS = [10, 100, 1000, 10000]                # number of training epochs \n",
    "N1     = [100, 1000, 6000]                     # training set size\n",
    "NODES  = [128, 256, 512, 1024, 2048, 4096]     # nodes in hidden layer\n",
    "ALPHA  = [0, 0.01, 10, 100, 1000, 10000]          # regularization parameter\n",
    "BS     = [10, 100, 1000]                       # batch size\n",
    "STD    = [0.01, 0.1, 1, 10]                    # weight initialization standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define TensorFlow computation graph\n",
    "class TNetwork:\n",
    "    def __init__(self, num_epochs, n1, nodes, alpha, batch_size, std):\n",
    "        # Save parameters\n",
    "        self.num_epochs = num_epochs\n",
    "        self.n1 = n1\n",
    "        self.batch_size = batch_size\n",
    "        self.alpha = alpha\n",
    "        \n",
    "        # Tensorflow computational graph\n",
    "        self.x = tf.placeholder(shape=[None, image_shape], dtype=tf.float32)\n",
    "        self.y = tf.placeholder(shape=[None, 2], dtype=tf.float32)\n",
    "        \n",
    "        self.W1 = tf.Variable(tf.truncated_normal([image_shape, nodes], stddev=std))\n",
    "        self.b1 = tf.Variable(tf.truncated_normal([nodes], stddev=std))\n",
    "        self.y1 = tf.nn.relu(tf.matmul(self.x, self.W1) + self.b1)\n",
    "        \n",
    "        self.W2 = tf.Variable(tf.truncated_normal([nodes, 2], stddev=std))\n",
    "        self.b2 = tf.Variable(tf.truncated_normal([2], stddev=std))\n",
    "        self.logits = tf.matmul(self.y1, self.W2) + self.b2\n",
    "        self.y_pred = tf.argmax(tf.nn.relu(self.logits), 1)\n",
    "        \n",
    "        self.l2 = alpha * (tf.nn.l2_loss(self.W1) + tf.nn.l2_loss(self.W2))\n",
    "        self.CE = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(self.logits, self.y))\n",
    "        self.loss_function = self.CE + tf.reduce_mean(self.l2)\n",
    "        self.optimizer = tf.train.AdamOptimizer().minimize(self.loss_function)\n",
    "        self.accuracy = 1.0 - tf.contrib.metrics.accuracy(self.y_pred, tf.argmax(self.y, 1))\n",
    "        \n",
    "        # Training Preparation\n",
    "        self.num_batches = np.ceil(self.n1 / self.batch_size)\n",
    "        \n",
    "    \n",
    "    def trainAllEpochs(self):\n",
    "        init = tf.initialize_all_variables()\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(init)\n",
    "            t_start = time.time()\n",
    "\n",
    "            print('%15s%20s%23s' % (' ','cross-entropy','error-rate'))\n",
    "            print('%15s%12s%12s%12s%12s%12s%12s' % ('epoch','training','validation','training','validation','L2','time (min)'))\n",
    "            for epoch in np.arange(self.num_epochs):\n",
    "                perm = np.random.permutation(self.n1)\n",
    "                ix = np.arange(0, self.batch_size)\n",
    "                epoch += 1\n",
    "                for batch in np.arange(self.num_batches):\n",
    "                    x_batch = inputs_train[perm[ix], :]\n",
    "                    y_batch = outputs_train[perm[ix], :]\n",
    "                    sess.run([self.optimizer],\n",
    "                             feed_dict={self.x: x_batch, self.y: y_batch})\n",
    "                    ix += self.batch_size\n",
    "                if (epoch % (self.num_epochs / 10)) == 0:\n",
    "                    CE_train, error_train, l2_loss = sess.run([self.CE, self.accuracy, self.l2],\n",
    "                                                 feed_dict={self.x: inputs_train,\n",
    "                                                            self.y: outputs_train})\n",
    "                    CE_valid, error_valid = sess.run([self.CE, self.accuracy],\n",
    "                                                 feed_dict={self.x: inputs_valid,\n",
    "                                                            self.y: outputs_valid})\n",
    "                    total_compute_time = (time.time() - t_start) / 60\n",
    "                    print('%7d %7d%12.5f%12.5f%12.3f%12.3f%12f%12.1f' \\\n",
    "                          % (self.num_epochs, epoch, CE_train, CE_valid,\n",
    "                             error_train, error_valid, l2_loss, total_compute_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      cross-entropy             error-rate\n",
      "          epoch    training  validation    training  validation          L2  time (min)\n",
      "     10       1     1.05243     1.10967       0.426       0.424    3.160516         0.1\n",
      "     10       2     0.66433     0.68982       0.386       0.414    2.051672         0.1\n",
      "     10       3     0.66159     0.68190       0.359       0.416    1.307331         0.2\n",
      "     10       4     0.65620     0.69289       0.384       0.399    0.842971         0.3\n",
      "     10       5     0.62418     0.66195       0.336       0.402    0.554878         0.4\n",
      "     10       6     0.60614     0.64584       0.334       0.374    0.373664         0.5\n",
      "     10       7     0.59531     0.65551       0.322       0.390    0.255127         0.5\n",
      "     10       8     0.58670     0.65380       0.303       0.391    0.178655         0.6\n",
      "     10       9     0.56369     0.64372       0.280       0.381    0.130747         0.7\n",
      "     10      10     0.54748     0.64140       0.267       0.372    0.102047         0.8\n"
     ]
    }
   ],
   "source": [
    "N1 = TNetwork(EPOCHS[0], N1[2], NODES[5], ALPHA[1], BS[2], STD[0])\n",
    "N1.trainAllEpochs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
