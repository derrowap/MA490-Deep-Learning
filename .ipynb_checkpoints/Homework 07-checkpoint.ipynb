{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 07\n",
    "Austin Derrow-Pinion CM 208"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_images shape: (40000, 28, 28)\n",
      "train_labels shape: (40000,)\n"
     ]
    }
   ],
   "source": [
    "# load CIFAR-10 Cats and Dogs split data set\n",
    "MNIST = np.load('./Data/MNIST_train_40000.npz')\n",
    "\n",
    "train_images = MNIST['train_images']\n",
    "train_labels = MNIST['train_labels']\n",
    "\n",
    "print('train_images shape: {}'.format(train_images.shape))\n",
    "print('train_labels shape: {}'.format(train_labels.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 10)\n"
     ]
    }
   ],
   "source": [
    "# prepare data\n",
    "image_size = 28 * 28\n",
    "input_images = np.reshape(np.array(train_images, dtype='float32'), [-1, image_size])\n",
    "input_labels = []\n",
    "\n",
    "labels = tf.constant(train_labels, tf.int32, [train_labels.size])\n",
    "one_hot_encode = tf.one_hot(labels, depth=10, on_value=1.0, off_value=0.0, dtype=tf.float32)\n",
    "init = tf.initialize_all_variables()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    input_labels = sess.run(one_hot_encode)\n",
    "print(input_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def minibatches(x, y, batchsize):\n",
    "    assert x.shape[0] == y.shape[0]\n",
    "    indices = np.arange(x.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    for i in range(0, x.shape[0] - batchsize + 1, batchsize):\n",
    "        excerpt = indices[i:i + batchsize]\n",
    "        yield np.array(x[excerpt], dtype='float32'), np.array(y[excerpt], dtype='int32')\n",
    "    \n",
    "    leftOver = (x.shape[0] - batchsize) % batchsize\n",
    "    if leftOver != 0:\n",
    "        excerpt = indices[len(indices) - leftOver:]\n",
    "        yield np.array(x[excerpt], dtype='float32'), np.array(y[excerpt], dtype='int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [None, image_size])\n",
    "y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "W1 = tf.Variable(tf.truncated_normal([image_size, 1024], dtype=tf.float32, seed=0, stddev=0.1))\n",
    "b1 = tf.Variable(tf.truncated_normal([1024], dtype=tf.float32, seed=0, stddev=0.1))\n",
    "y1 = tf.nn.relu(tf.matmul(x, W1) + b1)\n",
    "\n",
    "W2 = tf.Variable(tf.truncated_normal([1024, 10], dtype=tf.float32, seed=0, stddev=0.1))\n",
    "b2 = tf.Variable(tf.truncated_normal([10], dtype=tf.float32, seed=0, stddev=0.1))\n",
    "logits = tf.matmul(y1, W2) + b2\n",
    "\n",
    "CE = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(CE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stochastic_gradient_descent(sess, batch_size):\n",
    "    num_epochs = 0\n",
    "    batch_size = batch_size\n",
    "    total_elapsed_time = 0\n",
    "    CE_train = 1.0\n",
    "    num_CE_checks = 1\n",
    "    \n",
    "    while CE_train > 0.01:\n",
    "        num_epochs += 1\n",
    "        for batch in minibatches(input_images, input_labels, batch_size):\n",
    "            batch_x, batch_y = batch\n",
    "            \n",
    "            t_start = time.time()\n",
    "            _ = sess.run([optimizer], feed_dict={x: batch_x, y: batch_y})\n",
    "            elapsed_time = (time.time() - t_start) / 60\n",
    "            total_elapsed_time += elapsed_time\n",
    "            if total_elapsed_time / 0.1 >= num_CE_checks:\n",
    "                num_CE_checks += 1\n",
    "                CE_train = sess.run([CE], feed_dict={x: input_images, y: input_labels})[0]\n",
    "                print(\"Epoch = {}\".format(num_epochs))\n",
    "                print(\"\\tTraining CE = {:.3f}\".format(CE_train))\n",
    "                print(\"\\tTime passed = {:.1f}\".format(total_elapsed_time))\n",
    "            if CE_train < 0.01:\n",
    "                break\n",
    "    print(\"Finished Training with batch size = {}\".format(batch_size))\n",
    "    print(\"number of epochs = {}\".format(num_epochs))\n",
    "    print(\"final training cross-entropy = {:.3}\".format(CE_train))\n",
    "    print(\"total elapsed optimization time (minutes) = {:.3f}\".format(total_elapsed_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 1\n",
      "\tTraining CE = 0.142\n",
      "\tTime passed = 0.1\n",
      "Epoch = 1\n",
      "\tTraining CE = 0.122\n",
      "\tTime passed = 0.2\n",
      "Epoch = 2\n",
      "\tTraining CE = 0.095\n",
      "\tTime passed = 0.3\n",
      "Epoch = 2\n",
      "\tTraining CE = 0.066\n",
      "\tTime passed = 0.4\n",
      "Epoch = 3\n",
      "\tTraining CE = 0.045\n",
      "\tTime passed = 0.5\n",
      "Epoch = 3\n",
      "\tTraining CE = 0.102\n",
      "\tTime passed = 0.6\n",
      "Epoch = 4\n",
      "\tTraining CE = 0.033\n",
      "\tTime passed = 0.7\n",
      "Epoch = 4\n",
      "\tTraining CE = 0.026\n",
      "\tTime passed = 0.8\n",
      "Epoch = 4\n",
      "\tTraining CE = 0.027\n",
      "\tTime passed = 0.9\n",
      "Epoch = 5\n",
      "\tTraining CE = 0.039\n",
      "\tTime passed = 1.0\n",
      "Epoch = 5\n",
      "\tTraining CE = 0.016\n",
      "\tTime passed = 1.1\n",
      "Epoch = 6\n",
      "\tTraining CE = 0.033\n",
      "\tTime passed = 1.2\n",
      "Epoch = 6\n",
      "\tTraining CE = 0.045\n",
      "\tTime passed = 1.3\n",
      "Epoch = 7\n",
      "\tTraining CE = 0.016\n",
      "\tTime passed = 1.4\n",
      "Epoch = 7\n",
      "\tTraining CE = 0.018\n",
      "\tTime passed = 1.5\n",
      "Epoch = 7\n",
      "\tTraining CE = 0.038\n",
      "\tTime passed = 1.6\n",
      "Epoch = 8\n",
      "\tTraining CE = 0.013\n",
      "\tTime passed = 1.7\n",
      "Epoch = 8\n",
      "\tTraining CE = 0.019\n",
      "\tTime passed = 1.8\n",
      "Epoch = 9\n",
      "\tTraining CE = 0.016\n",
      "\tTime passed = 1.9\n",
      "Epoch = 9\n",
      "\tTraining CE = 0.018\n",
      "\tTime passed = 2.0\n",
      "Epoch = 10\n",
      "\tTraining CE = 0.014\n",
      "\tTime passed = 2.1\n",
      "Epoch = 10\n",
      "\tTraining CE = 0.045\n",
      "\tTime passed = 2.2\n",
      "Epoch = 10\n",
      "\tTraining CE = 0.014\n",
      "\tTime passed = 2.3\n",
      "Epoch = 11\n",
      "\tTraining CE = 0.010\n",
      "\tTime passed = 2.4\n",
      "Finished Training with batch size = 10\n",
      "number of epochs = 11\n",
      "final training cross-entropy = 0.00986\n",
      "total elapsed optimization time (minutes) = 2.400\n"
     ]
    }
   ],
   "source": [
    "init = tf.initialize_all_variables()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    stochastic_gradient_descent(sess, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 4\n",
      "\tTraining CE = 0.028\n",
      "\tTime passed = 0.1\n",
      "Epoch = 7\n",
      "\tTraining CE = 0.007\n",
      "\tTime passed = 0.2\n",
      "Finished Training with batch size = 100\n",
      "number of epochs = 7\n",
      "final training cross-entropy = 0.00723\n",
      "total elapsed optimization time (minutes) = 0.200\n"
     ]
    }
   ],
   "source": [
    "init = tf.initialize_all_variables()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    stochastic_gradient_descent(sess, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 9\n",
      "\tTraining CE = 0.044\n",
      "\tTime passed = 0.1\n",
      "Epoch = 18\n",
      "\tTraining CE = 0.012\n",
      "\tTime passed = 0.2\n",
      "Epoch = 26\n",
      "\tTraining CE = 0.005\n",
      "\tTime passed = 0.3\n",
      "Finished Training with batch size = 1000\n",
      "number of epochs = 26\n",
      "final training cross-entropy = 0.00505\n",
      "total elapsed optimization time (minutes) = 0.300\n"
     ]
    }
   ],
   "source": [
    "init = tf.initialize_all_variables()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    stochastic_gradient_descent(sess, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 9\n",
      "\tTraining CE = 0.250\n",
      "\tTime passed = 0.1\n",
      "Epoch = 19\n",
      "\tTraining CE = 0.153\n",
      "\tTime passed = 0.2\n",
      "Epoch = 29\n",
      "\tTraining CE = 0.108\n",
      "\tTime passed = 0.3\n",
      "Epoch = 38\n",
      "\tTraining CE = 0.079\n",
      "\tTime passed = 0.4\n",
      "Epoch = 48\n",
      "\tTraining CE = 0.059\n",
      "\tTime passed = 0.5\n",
      "Epoch = 57\n",
      "\tTraining CE = 0.045\n",
      "\tTime passed = 0.6\n",
      "Epoch = 66\n",
      "\tTraining CE = 0.034\n",
      "\tTime passed = 0.7\n",
      "Epoch = 76\n",
      "\tTraining CE = 0.027\n",
      "\tTime passed = 0.8\n",
      "Epoch = 85\n",
      "\tTraining CE = 0.021\n",
      "\tTime passed = 0.9\n",
      "Epoch = 95\n",
      "\tTraining CE = 0.017\n",
      "\tTime passed = 1.0\n",
      "Epoch = 105\n",
      "\tTraining CE = 0.014\n",
      "\tTime passed = 1.1\n",
      "Epoch = 115\n",
      "\tTraining CE = 0.011\n",
      "\tTime passed = 1.2\n",
      "Epoch = 125\n",
      "\tTraining CE = 0.009\n",
      "\tTime passed = 1.3\n",
      "Finished Training with batch size = 10000\n",
      "number of epochs = 125\n",
      "final training cross-entropy = 0.00921\n",
      "total elapsed optimization time (minutes) = 1.301\n"
     ]
    }
   ],
   "source": [
    "init = tf.initialize_all_variables()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    stochastic_gradient_descent(sess, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
