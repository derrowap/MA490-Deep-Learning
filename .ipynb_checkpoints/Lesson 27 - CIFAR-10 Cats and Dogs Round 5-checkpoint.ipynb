{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 27 - CIFAR-10 Cats and Dogs Round 5\n",
    "Austin Derrow-Pinion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.metrics import accuracy_score\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CIFAR-10 cats and dogs training and validation data set loaded.\n",
      "  training input: (6000, 32, 32, 3)  output: (6000,)\n",
      "validation input: (2000, 32, 32, 3)  output: (2000,)\n"
     ]
    }
   ],
   "source": [
    "# load training, validation and testing data sets\n",
    "CIFAR10 = np.load('./Data/CIFAR-10_cats_dogs_split.npz')\n",
    "label_names  = CIFAR10['label_names']\n",
    "images_train = CIFAR10['images_train']\n",
    "labels_train = CIFAR10['labels_train']\n",
    "images_valid = CIFAR10['images_valid']\n",
    "labels_valid = CIFAR10['labels_valid']\n",
    "\n",
    "print('CIFAR-10 cats and dogs training and validation data set loaded.')\n",
    "print('  training input:',images_train.shape,' output:',labels_train.shape)\n",
    "print('validation input:',images_valid.shape,' output:',labels_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reshape image tensors to a matrix \n",
    "image_shape = 32*32*3\n",
    "inputs_train = images_train.reshape(-1,image_shape)\n",
    "inputs_valid = images_valid.reshape(-1,image_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# scale data\n",
    "inputs_train = scale(inputs_train.astype('float64'),axis=1)\n",
    "inputs_valid = scale(inputs_valid.astype('float64'),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data reshaped, scaled and one-hot-encoded:\n",
      "   training input: (6000, 3072)  output: (6000, 2)\n",
      " validation input: (2000, 3072)  output: (2000, 2)\n"
     ]
    }
   ],
   "source": [
    "# one-hot-encode labels\n",
    "outputs_train = OneHotEncoder(sparse=False).fit_transform(labels_train.reshape(-1,1))\n",
    "outputs_valid = OneHotEncoder(sparse=False).fit_transform(labels_valid.reshape(-1,1))\n",
    "\n",
    "print('Data reshaped, scaled and one-hot-encoded:')\n",
    "print('   training input:',inputs_train.shape,' output:',outputs_train.shape)\n",
    "print(' validation input:',inputs_valid.shape,' output:',outputs_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hyper-parameters\n",
    "std = 0.1\n",
    "batch_size = 1000\n",
    "epochs = 100\n",
    "alpha = 0.01\n",
    "training_data_size = 6000\n",
    "\n",
    "# nodes in each hidden layer\n",
    "n1 = 3072\n",
    "n2 = 1536\n",
    "n3 = 768\n",
    "n4 = 384\n",
    "n5 = 192\n",
    "n6 = 96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Tensorflow computational graph\n",
    "x = tf.placeholder(shape=[None, image_shape], dtype=tf.float32)\n",
    "y = tf.placeholder(shape=[None, 2], dtype=tf.float32)\n",
    "\n",
    "# input --> hidden layer 1\n",
    "W1 = tf.Variable(tf.truncated_normal([image_shape, n1], stddev=std))\n",
    "b1 = tf.Variable(tf.truncated_normal([n1], stddev=std))\n",
    "y1 = tf.nn.relu(tf.matmul(x, W1) + b1)\n",
    "\n",
    "# hidden layer 1 --> hidden layer 2\n",
    "W2 = tf.Variable(tf.truncated_normal([n1, n2], stddev=std))\n",
    "b2 = tf.Variable(tf.truncated_normal([n2], stddev=std))\n",
    "y2 = tf.nn.relu(tf.matmul(y1, W2) + b2)\n",
    "\n",
    "# hidden layer 2 --> hidden layer 3\n",
    "W3 = tf.Variable(tf.truncated_normal([n2, n3], stddev=std))\n",
    "b3 = tf.Variable(tf.truncated_normal([n3], stddev=std))\n",
    "y3 = tf.nn.relu(tf.matmul(y2, W3) + b3)\n",
    "\n",
    "# hidden layer 3 --> hidden layer 4\n",
    "W4 = tf.Variable(tf.truncated_normal([n3, n4], stddev=std))\n",
    "b4 = tf.Variable(tf.truncated_normal([n4], stddev=std))\n",
    "y4 = tf.nn.relu(tf.matmul(y3, W4) + b4)\n",
    "\n",
    "# hidden layer 4 --> hidden layer 5\n",
    "W5 = tf.Variable(tf.truncated_normal([n4, n5], stddev=std))\n",
    "b5 = tf.Variable(tf.truncated_normal([n5], stddev=std))\n",
    "y5 = tf.nn.relu(tf.matmul(y4, W5) + b5)\n",
    "\n",
    "# hidden layer 5 --> hidden layer 6\n",
    "W6 = tf.Variable(tf.truncated_normal([n5, n6], stddev=std))\n",
    "b6 = tf.Variable(tf.truncated_normal([n6], stddev=std))\n",
    "y6 = tf.nn.relu(tf.matmul(y5, W6) + b6)\n",
    "\n",
    "# hidden layer 6 --> output\n",
    "W7 = tf.Variable(tf.truncated_normal([n6, 2], stddev=std))\n",
    "b7 = tf.Variable(tf.truncated_normal([2], stddev=std))\n",
    "logits = tf.matmul(y6, W7) + b7\n",
    "y_pred = tf.argmax(tf.nn.softmax(logits), 1)\n",
    "\n",
    "l2 = alpha * tf.reduce_mean(tf.nn.l2_loss(W1)\n",
    "                                + tf.nn.l2_loss(W2)\n",
    "                                + tf.nn.l2_loss(W3)\n",
    "                                + tf.nn.l2_loss(W4)\n",
    "                                + tf.nn.l2_loss(W5)\n",
    "                                + tf.nn.l2_loss(W6)\n",
    "                                + tf.nn.l2_loss(W7))\n",
    "\n",
    "CE = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, y))\n",
    "loss_function = CE + l2\n",
    "optimizer = tf.train.AdamOptimizer().minimize(loss_function)\n",
    "accuracy = 1.0 - tf.contrib.metrics.accuracy(y_pred, tf.argmax(y, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trainNetwork(sess, init):\n",
    "    sess.run(init)\n",
    "    t_start = time.time()\n",
    "    num_batches = np.ceil(training_data_size / batch_size)\n",
    "    print('%15s%20s%23s' % (' ','cross-entropy','error-rate'))\n",
    "    print('%15s%12s%12s%12s%12s%12s%12s' % ('epoch','training','validation','training','validation','L2','time (min)'))\n",
    "    for e in np.arange(epochs):\n",
    "        perm = np.random.permutation(training_data_size)\n",
    "        ix = np.arange(0, batch_size)\n",
    "        e += 1\n",
    "        for batch in np.arange(num_batches):\n",
    "            x_batch = inputs_train[perm[ix], :]\n",
    "            y_batch = outputs_train[perm[ix], :]\n",
    "            sess.run([optimizer],\n",
    "                     feed_dict={x: x_batch, y: y_batch})\n",
    "            ix += batch_size\n",
    "        if (e % (epochs / 10)) == 0:\n",
    "            CE_train, error_train, l2_loss = sess.run([CE, accuracy, l2],\n",
    "                                         feed_dict={x: inputs_train,\n",
    "                                                    y: outputs_train})\n",
    "            CE_valid, error_valid = sess.run([CE, accuracy],\n",
    "                                         feed_dict={x: inputs_valid,\n",
    "                                                    y: outputs_valid})\n",
    "            total_compute_time = (time.time() - t_start) / 60\n",
    "            print('%7d %7d%12.5f%12.5f%12.3f%12.3f%12f%12.1f' \\\n",
    "                  % (epochs, e, CE_train, CE_valid,\n",
    "                     error_train, error_valid, l2_loss, total_compute_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      cross-entropy             error-rate\n",
      "          epoch    training  validation    training  validation          L2  time (min)\n",
      "    100      10     0.32039     0.78572       0.115       0.405  451.260345         0.2\n",
      "    100      20     0.16682     0.74645       0.004       0.387  292.334869         0.5\n",
      "    100      30     0.10792     0.75965       0.001       0.376  185.587189         0.7\n",
      "    100      40     0.11126     0.91926       0.022       0.396  120.481316         0.9\n",
      "    100      50     0.02598     0.90964       0.000       0.368   81.518486         1.1\n",
      "    100      60     0.27104     0.66733       0.045       0.386   57.918484         1.4\n",
      "    100      70     0.17721     0.80739       0.047       0.383   42.728481         1.6\n",
      "    100      80     0.32156     0.72466       0.104       0.381   32.641914         1.8\n",
      "    100      90     0.04413     0.96471       0.001       0.380   25.596832         2.0\n",
      "    100     100     0.04095     0.96667       0.004       0.360   20.583456         2.3\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "init = tf.initialize_all_variables()\n",
    "trainNetwork(sess, init)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
